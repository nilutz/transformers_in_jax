# Transformer Resources

# 0. catalog
* [Transformer models: an introduction and catalog](https://arxiv.org/pdf/2302.07730.pdf)
# 1. Vanilla
* [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
* [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

### 1. Vanilla implementations
* https://github.com/bhavnicksm/vanilla-transformer-jax/tree/main
* http://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks
* https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html#The-Transformer-architecture
* https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/models.py

# 2. Bert
* https://github.com/nikitakit/sabertooth
* https://arxiv.org/pdf/1810.04805.pdf
* https://github.com/jessevig/bertviz
* https://github.com/google/flaxformer
* https://arxiv.org/abs/1810.04805
* https://arxiv.org/abs/2212.14034

* https://github.com/samvher/bert-for-laptops/blob/main/BERT_for_laptops.ipynb
* https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch

+ from pytorch: https://github.com/Cli212/flax-BERT/blob/master/bert.py#L168

Preprocessing:
* https://keras.io/examples/nlp/pretraining_BERT/
* https://github.com/huggingface/transformers/blob/3dcb748e31be8c7c9e4f62926c5c144c62d07218/src/transformers/data/data_collator.py#L169

# 3.


# 4. GPT
* https://github.com/brentyi/minGPT-flax/blob/master/mingpt/trainer.py
* https://github.com/karpathy/nanoGPT
* https://github.com/jenkspt/gpt-jax/blob/main/model.py

# 4.5 Lama
* https://huggingface.co/erfanzar/Llama-2-jax/blob/main/model.py

# 5. RLHF / InstructGPT

# 6 t5
* https://github.com/google-research/text-to-text-transfer-transformer